{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron \n",
    "\n",
    "The task of this exercise is to reimplement and test the perceptron algorithm from UE 06 for verifying the authenticity of banknotes. Rewrite the program and replace basic Python with NumPy as much as possible. \n",
    "\n",
    "A revised version of the Perceptron algorithm from UE 06 is shown below. Use the dataset `./data/banknote.csv`. To read the data into a NumPy array call \n",
    "\n",
    "```python\n",
    "    Z = np.loadtxt('./data/banknote.csv', delimiter=',')\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task**\n",
    "\n",
    "+ Load the data\n",
    "\n",
    "+ Display information about the data set (see below)\n",
    "\n",
    "+ Randomly split the data into a training set (80 %) and test set (20 %)\n",
    "\n",
    "+ Train a model using the Perceptron algorithm on the training set\n",
    "\n",
    "+ Evaluate the error rate of the trained model on the test set\n",
    "\n",
    "<br>\n",
    "\n",
    "**Note:** Before training a model, it is important to explore the data you want to learn from. Display some general statistics such as the number of examples, number of features, number of classes, mean, standard deviation, minimum, and maximum of each feature, and so on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### Perceptron Algorithm\n",
    "\n",
    "The perceptron algorithm described here assumes class labels of $\\pm 1$. A feature vector is denoted by $\\mathbf{x} \\in \\mathbb{R}^n$ and its corresponding class label by $y \\in \\{\\pm 1\\}$.\n",
    "\n",
    "A perceptron with weight vector $\\mathbf{w} \\in \\mathbb{R}^n$ and bias $b \\in \\mathbb{R}$ classifies a point $\\mathbf{x} \\in \\mathbb{R}^n$ to class $y$ according to the rule\n",
    "\n",
    "$$\n",
    "    f_{\\mathbf{w}, b}(\\mathbf{x}) = \\begin{cases}\n",
    "    +1 & \\mathbf{w}^{t}\\mathbf{x} + b \\geq 0\\\\\n",
    "    -1 & \\text{otherwise}\n",
    "    \\end{cases},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}^{t}\\mathbf{x} = w_1x_1 + \\cdots + w_n x_n$ is the dot product of $\\mathbf{w}$ and $\\mathbf{x}$. The prediction $f_{\\mathbf{w}, b}(\\mathbf{x})$ is correct if $f_{\\mathbf{w}, b}(\\mathbf{x}) = y$. Otherwise, there is a misclassification.\n",
    "\n",
    "The goal of learning is to find suitable parameters $\\mathbf{w}$ and $b$ such that the learned function $f_{\\mathbf{w}, b}$ misclassifies as few training examples as possible. \n",
    "\n",
    "The perceptron algorithm describes how suitable parameters $\\mathbf{w}$ and $b$ can be learned.\n",
    "\n",
    "**Perceptron Algorithm**\n",
    "\n",
    "1. Initialize the weight vector $\\mathbf{w}$ and bias $b$\n",
    "2. Repeat for `max_iter` times\n",
    "    1. For each training example $(\\mathbf{x}, y)$:\n",
    "        1. calculate $\\hat{y} = f_{\\mathbf{w}, b}(\\mathbf{x})$\n",
    "        2. update $\\mathbf{w}$ and $b$ according to the rule\n",
    "        \n",
    "            $\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta(y-\\hat{y}) \\mathbf{x}$\n",
    "\n",
    "            $b \\leftarrow b + \\eta(y-\\hat{y})$\n",
    "        \n",
    "        where $\\eta \\in [0, 1]$ is the learning rate (step size).\n",
    "    2. Output the classification accuracy over all training examples\n",
    "3. Return $\\mathbf{w}$ and $b$.\n",
    "\n",
    "In **Step 1**, the weights $\\mathbf{w}$ and the bias $b$ are initialized with random values between $[-\\sigma, +\\sigma]$. Use a suitable NumPy function for this purpose. In **Step 2**, the algorithm is repeated for a predetermined number of iterations (`max_iter`). In each iteration, all training examples are visited, the prediction is calculated, and the weight vector and bias are updated. After each iteration, the classification accuracy on the training set is calculated and displayed. In **Step 3**, the learned weight vector and bias are returned.\n",
    "\n",
    "You can use the following parameter setting: \n",
    "\n",
    "```\n",
    "    sigma = 0.1\t\n",
    "    num_epochs = 10\t\n",
    "    eta = 0.1\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines/data samples:  1372\n",
      "number of classes:  2\n",
      "info for collumn 1 :\n",
      "mean:  0.4337352570699707\n",
      "standart deviation:  2.841726405239481\n",
      "min:  -7.0421\n",
      "max:  6.8248\n",
      "info for collumn 2 :\n",
      "mean:  1.9223531206393585\n",
      "standart deviation:  5.866907488387089\n",
      "min:  -13.7731\n",
      "max:  12.9516\n",
      "info for collumn 3 :\n",
      "mean:  1.3976271172667638\n",
      "standart deviation:  4.308459093119795\n",
      "min:  -5.2861\n",
      "max:  17.9274\n",
      "info for collumn 4 :\n",
      "mean:  -1.1916565200437317\n",
      "standart deviation:  2.100247322449037\n",
      "min:  -8.5482\n",
      "max:  2.4495\n",
      "info for collumn 5 :\n",
      "mean:  0.4446064139941691\n",
      "standart deviation:  0.49692207701954094\n",
      "min:  0.0\n",
      "max:  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "filename = 'data/banknote.csv'\n",
    "\n",
    "data = np.loadtxt(filename, delimiter = \",\")\n",
    "\n",
    "def info_data(X):\n",
    "    print(f'{\"number of lines/data samples: \"} {X.shape[0]:>}')\n",
    "    print(f'{\"number of classes: \"} {len(np.unique(X[:,4])):>}')\n",
    "    for i in range(5):\n",
    "        print(f'{\"info for collumn\"} {i+1} {\":\"}')\n",
    "        print(f'{\"mean: \"} {np.mean(X[:,i])}')\n",
    "        print(f'{\"standart deviation: \"} {np.std(X[:,i])}')\n",
    "        print(f'{\"min: \"} {np.min(X[:,i])}')\n",
    "        print(f'{\"max: \"} {np.max(X[:,i])}')\n",
    "\n",
    "info_data(data)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, train_size=0.8, random_state=42) #randomly splits the data if random_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,1) (1097,1097) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m weights, bias \u001b[38;5;241m=\u001b[39m init_values()\n\u001b[0;32m     22\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m calc_y(weights, bias, train_data[:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[80], line 17\u001b[0m, in \u001b[0;36mupdate\u001b[1;34m(weights, bias, X, y_pred)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(weights, bias, X, y_pred):\n\u001b[1;32m---> 17\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#wie zum fick soll des gehen?\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     bias \u001b[38;5;241m=\u001b[39m bias \u001b[38;5;241m+\u001b[39m eta\u001b[38;5;241m*\u001b[39m(X[:,\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m-\u001b[39m y_pred)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (weights, bias)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,1) (1097,1097) "
     ]
    }
   ],
   "source": [
    "#starting values:\n",
    "sigma = 0.1\n",
    "num_epochs = 10\n",
    "eta = 0.1\n",
    "\n",
    "def init_values():\n",
    "    weights = np.random.uniform(low=-sigma, high=sigma, size=(len(data[0,:])-1, 1))\n",
    "    bias = np.random.uniform(low=-sigma, high=sigma)\n",
    "    return (weights, bias)\n",
    "\n",
    "def calc_y(weights, bias, data):\n",
    "    y_pred = np.dot(data, weights) + bias\n",
    "    y_pred = np.where(y_pred >= 0, 1, -1)\n",
    "    return y_pred\n",
    "\n",
    "def update(weights, bias, X, y_pred):\n",
    "    weights = weights + eta*(X[:,4] - y_pred) * np.mean(X[:,0:4], axis=1) #wie zum fick soll des gehen?\n",
    "    bias = bias + eta*(X[:,4] - y_pred)\n",
    "    return (weights, bias)\n",
    "\n",
    "weights, bias = init_values()\n",
    "y_pred = calc_y(weights, bias, train_data[:,0:4])\n",
    "update(weights, bias, train_data, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
